{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cebuano Stemmer Demo\n",
    "\n",
    "`Accepts a Cebuano word and returns the root word and its affixes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a Cebuano word: dagkong\n",
      "dagkong:[(dagko), {-,-,ng}]\n"
     ]
    }
   ],
   "source": [
    "from stemmer import stem_word\n",
    "\n",
    "ceb_word = raw_input('Input a Cebuano word: ')\n",
    "word = stem_word(word=ceb_word)\n",
    "\n",
    "print(word.print_stem_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cebuano POS Tagger Demo\n",
    "\n",
    "`Accepts a Cebuano sentence and return POS tagged sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a Cebuano sentence: Patay ang usa ka security guard nga hubog nagmaniho sa iyang motorsiklo ug  nakabangga sa gikasugat nga laing motorsiklo atol sa Pasko sa national highway sa Barangay Fuente, lungsod sa Carmen.\n",
      "Patay/NOUN ang/PART usa/NOUN ka/PART security/NOUN guard/NOUN nga/PART hubog/ADJ nagmaniho/VERB sa/PART iyang/PRON motorsiklo/NOUN ug/CONJ nakabangga/VERB sa/PART gikasugat/OTH nga/PART laing/ADJ motorsiklo/NOUN atol/PART sa/PART Pasko/NOUN sa/PART national/VERB highway/NOUN sa/PART Barangay/NOUN Fuente/NOUN ,/SYM lungsod/NOUN sa/PART Carmen/NOUN ./SYM \n"
     ]
    }
   ],
   "source": [
    "from tagger import tag_sentence\n",
    "\n",
    "ceb_sentence = raw_input('Input a Cebuano sentence: ')\n",
    "words = tag_sentence(text=ceb_sentence)\n",
    "\n",
    "sentence = ''\n",
    "for word in words:\n",
    "    sentence += str(word) + ' '\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer Evaluation\n",
    "\n",
    "\n",
    "`Evaluates the stemmer by getting the number of predicted root words vs. actual root words`\n",
    "\n",
    "* The input words are already tokenized and stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1298\n",
      "Correct root words: 955\n",
      "Incorrect root words: 343\n",
      "Correct / incorrect root percentage: 73.57473035439138\n",
      "\n",
      "\n",
      "Dictionary entry tokens: 974\n",
      "Correct dictionary entry tokens: 878\n",
      "Incorrect dictionary entry tokens: 96\n",
      "\n",
      "\n",
      "Non-dictionary entry tokens: 324\n",
      "Correct non-dictionary entry tokens: 77\n",
      "Incorrect non-dictionary entry tokens: 247\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>infix</th>\n",
       "      <th>is_entry</th>\n",
       "      <th>is_root</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>prefix</th>\n",
       "      <th>root</th>\n",
       "      <th>suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nipahibawo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ni</td>\n",
       "      <td>pahibaw</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naghuwat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>nag</td>\n",
       "      <td>huwat</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsaon</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsa</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mamahimong</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ma</td>\n",
       "      <td>himo</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syudad</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>syudad</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sirhan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sir</td>\n",
       "      <td>han</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kalapasan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ka</td>\n",
       "      <td>lapas</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bawian</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ba</td>\n",
       "      <td>wi</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nakalapas</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>naka</td>\n",
       "      <td>lapas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nagkadaiyang</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nag</td>\n",
       "      <td>kadaiya</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             infix  is_entry  is_root  is_valid prefix     root suffix\n",
       "nipahibawo     NaN     False    False     False     ni  pahibaw      o\n",
       "naghuwat       NaN      True    False      True    nag    huwat    NaN\n",
       "unsaon         NaN      True    False      True    NaN     unsa     on\n",
       "mamahimong     NaN      True    False      True     ma     himo     ng\n",
       "syudad         NaN      True     True      True    NaN   syudad    NaN\n",
       "sirhan         NaN      True    False     False    NaN      sir    han\n",
       "kalapasan      NaN      True    False      True     ka    lapas     an\n",
       "bawian         NaN     False    False     False     ba       wi     an\n",
       "nakalapas      NaN      True    False      True   naka    lapas    NaN\n",
       "nagkadaiyang   NaN      True    False     False    nag  kadaiya     ng"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stemmer_evaluator as se\n",
    "import pandas as pd\n",
    "\n",
    "result = se.to_panda_data()\n",
    "\n",
    "df = pd.DataFrame(result['data'], index=result['index'])\n",
    "df_valid = df[(df.is_valid == True)]\n",
    "valid = df_valid.count()['is_valid']\n",
    "print('Total tokens: ' + str(df.count()['is_valid']))\n",
    "print('Correct root words: ' + str(df[(df.is_valid == True)].count()['is_valid']))\n",
    "print('Incorrect root words: ' + str(df[(df.is_valid == False)].count()['is_valid']))\n",
    "print(\"Correct / incorrect root percentage: \" + str((valid / float(df.count()['is_valid'])) * 100))\n",
    "print('\\n')\n",
    "print('Dictionary entry tokens: ' + str(df[(df.is_entry == True)].count()['is_entry']))\n",
    "print('Correct dictionary entry tokens: ' + str(df[(df.is_entry == True) & (df.is_valid == True)].count()['is_entry']))\n",
    "print('Incorrect dictionary entry tokens: ' + str(df[(df.is_entry == True) & (df.is_valid == False)].count()['is_entry']))\n",
    "print('\\n')\n",
    "print('Non-dictionary entry tokens: ' + str(df[(df.is_entry == False)].count()['is_entry']))\n",
    "print('Correct non-dictionary entry tokens: ' + str(df[(df.is_entry == False) & (df.is_valid == True)].count()['is_entry']))\n",
    "print('Incorrect non-dictionary entry tokens: ' + str(df[(df.is_entry == False) & (df.is_valid == False)].count()['is_entry']))\n",
    "print('\\n')\n",
    "\n",
    "df.tail(n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger Evaluation\n",
    "\n",
    "\n",
    "`Evaluates the tagger by getting the number of predicted pos tags vs. actual pos tags (F-score)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 236\n",
      "\n",
      "Disambiguated: 100.0% (236) \n",
      "\n",
      "Predicted  ADJ  ADV  CONJ  DET  NOUN  NUM  OTH  PART  PRON  SYM  VERB  __all__\n",
      "Actual                                                                        \n",
      "ADJ          2    1     0    0     0    0    1     0     0    0     0        4\n",
      "ADV          0    1     2    0     1    0    1     0     0    0     0        5\n",
      "CONJ         0    0     4    0     0    0    0     0     0    0     0        4\n",
      "DET          0    0     9   31     0    0    0     1     0    0     0       41\n",
      "NOUN         6    0     0    0    43    0    2     0     3    0     4       58\n",
      "NUM          0    0     0    0     0    3    0     0     0    0     0        3\n",
      "OTH          0    0     0    0     0    0    0     0     0    0     0        0\n",
      "PART         0    4     2    0     0    0    0    17     2    0     0       25\n",
      "PRON         0    0     0    0     1    0    0     0    26    0     0       27\n",
      "SYM          0    0     0    0     0    0    0     0     0   37     0       37\n",
      "VERB         1    0     0    0     5    0    0     0     0    0    26       32\n",
      "__all__      9    6    17   31    50    3    4    18    31   37    30      236\n",
      "\n",
      "Overall F-Score\n",
      "\n",
      "0.8050847457627118\n",
      "\n",
      "\n",
      "Individual F-Score\n",
      "\n",
      "ADJ  : 0.30769230769230765\n",
      "ADV  : 0.1818181818181818\n",
      "CONJ : 0.38095238095238093\n",
      "DET  : 0.8611111111111112\n",
      "NOUN : 0.7962962962962963\n",
      "NUM  : 1.0\n",
      "OTH  : 0.0\n",
      "PART : 0.7906976744186047\n",
      "PRON : 0.896551724137931\n",
      "SYM  : 1.0\n",
      "VERB : 0.8387096774193549\n",
      "0.7053829353846168\n"
     ]
    }
   ],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.metrics import f1_score\n",
    "import tagger_evaluator as te\n",
    "\n",
    "words = te.tag_test_sentences()\n",
    "y_true = te.extract_actual_pos_tags()\n",
    "y_pred = te.extract_predicted_pos_tags(words=words)\n",
    "confusion_matrix = ConfusionMatrix(y_true, y_pred)\n",
    "# confusion_matrix.print_stats()\n",
    "print(confusion_matrix)\n",
    "\n",
    "print('\\nOverall F-Score\\n')\n",
    "f1score = f1_score(y_true, y_pred, average='micro')\n",
    "print(f1score)\n",
    "print('\\n')\n",
    "\n",
    "df = f1_score(y_true, y_pred, average=None)\n",
    "print('Individual F-Score' + '\\n')\n",
    "print('ADJ  : ' + str(df[0]))\n",
    "print('ADV  : ' + str(df[1]))\n",
    "print('CONJ : ' + str(df[2]))\n",
    "print('DET  : ' + str(df[3]))\n",
    "print('NOUN : ' + str(df[4]))\n",
    "print('NUM  : ' + str(df[5]))\n",
    "print('OTH  : ' + str(df[6]))\n",
    "print('PART : ' + str(df[7]))\n",
    "print('PRON : ' + str(df[8]))\n",
    "print('SYM  : ' + str(df[9]))\n",
    "print('VERB : ' + str(df[10]))\n",
    "\n",
    "f_score = 0\n",
    "for i in range(0, 11):\n",
    "    if i != 6:\n",
    "        f_score += df[i]\n",
    "\n",
    "print(f_score / 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
